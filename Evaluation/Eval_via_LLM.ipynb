{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating All Model Responses via Groq Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(filename):\n",
    "    with open(filename) as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Folder for Saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Evaluation/Eval_Results'):\n",
    "    os.makedirs('Evaluation/Eval_Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Judge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model = '' # One of ['llama3-70b-8192', 'mixtral-8x7b-32768', 'gpt-4-turbo'] in our case\n",
    "eval_method = '' # One of ['Groq', 'OpenAI] in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request(query):\n",
    "    if eval_method == 'OpenAI':\n",
    "        client = OpenAI(\n",
    "            api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": query,\n",
    "                }\n",
    "            ],\n",
    "            model=judge_model,\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    elif eval_method == 'Groq':\n",
    "        GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "        client = Groq(api_key=GROQ_API_KEY)\n",
    "        chat_hist = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "                messages=chat_hist,\n",
    "                model=judge_model,\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid eval_method: {eval_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Model to Eval and Load Its Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = '' # One of ['llama3-8b-8192', 'llama3-70b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it', 'gemini-1.5-flash', 'gpt-3.5-turbo', 'gpt-4-turbo'] in our case\n",
    "mode = '' # One of ['normal', 'cot'] in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response_path = f'LLM_Inference/Saves/ls_preds_{eval_model}_{mode}.yaml'\n",
    "\n",
    "model_responses = read_jsonl_file(model_response_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_variable_predictions = \"\"\"You are the host of a quiz show where you ask complex and tricky questions to the contestants. \n",
    "Now once you ask one such question the contestant gives an answer which might not be the exact answer but is still correct for instance the answer provided to you might be 'U.S.A' while the actual answer is 'United States of America' or 'United States' or 'America' etc.\n",
    "Use your wise judgement to decide based on the question given whether the answer is correct or not. YOU ARE THE JUDGE AND YOUR WORD IS FINAL. Be fair and just in your judgement. Always respond with 'correct' or 'incorrect' based on the answer provided by the contestant. \n",
    "You will be provided the question, true answer and the answer provided by the contestant and you need to decide whether the answer is correct or not.\n",
    "\n",
    "## Question\n",
    "<question>\n",
    "\n",
    "## True Answer\n",
    "<true_answer>\n",
    "\n",
    "## Answer Given by contestant\n",
    "<answer_given_by_contestant>\n",
    "\n",
    "## Your Judgement (correct/incorrect)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_rationale_predictions = \"\"\"You are the host of a quiz show where you ask complex and tricky questions to the contestants. \n",
    "Now once you ask one such question the contestant gives an answer as well as the rationale behind that answer. You need to decide whether the rationale provided is correct or not by comparing it with the true rationale.\n",
    "Use your wise judgement to decide based on the question given whether the rationale is correct or not. YOU ARE THE JUDGE AND YOUR WORD IS FINAL. Be fair and just in your judgement. Provide a score between 1 to 5 based on the rationale provided by the contestant.\n",
    "1 being the least and 5 being the highest score.\n",
    "\n",
    "## Question\n",
    "<question>\n",
    "\n",
    "## True Rationale\n",
    "<true_answer>\n",
    "\n",
    "## Rationale Given by contestant\n",
    "<rationale_given_by_contestant>\n",
    "\n",
    "## Your Judgement (Score between 1 to 5 do not provide any other score or text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer(question, model_answer, actual_answer):\n",
    "    query = prompt_for_variable_predictions.replace('<question>', question).replace('<true_answer>', actual_answer).replace('<answer_given_by_contestant>', model_answer)\n",
    "    response = api_request(query)\n",
    "    return response\n",
    "\n",
    "def eval_rationale(question, model_rationale, actual_rationale):\n",
    "    query = prompt_for_rationale_predictions.replace('<question>', question).replace('<true_answer>', actual_rationale).replace('<rationale_given_by_contestant>', model_rationale)\n",
    "    response = api_request(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'Evaluation/Eval_Results/Eval_Model_{eval_model}_Judge_Model_{judge_model}_Mode_{mode}'):\n",
    "    os.makedirs(f'Evaluation/Eval_Results/Eval_Model_{eval_model}_Judge_Model_{judge_model}_Mode_{mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in model_responses:\n",
    "    file_name = list(response.keys())[0]\n",
    "    response = response[file_name]\n",
    "    ls_var_ans_preds = response['ls_var_ans_preds']\n",
    "    ls_var_rationale = response['ls_var_rationale']\n",
    "    ls_var_gold_rationales = response['ls_var_gold_rationales']\n",
    "\n",
    "    data_point_file_path = f'Data_Store/{file_name}.yml'\n",
    "    data_point = load_yaml(data_point_file_path)\n",
    "\n",
    "    variables = data_point['variables']\n",
    "    variable_to_answer = data_point['variable_to_answer']\n",
    "    variable_specific_rationale = data_point['variable_specific_rationale']\n",
    "    question = data_point['question']\n",
    "\n",
    "    for var in variables:\n",
    "        ans_for_var = variable_to_answer[var]\n",
    "        rationale_for_var = variable_specific_rationale[var]\n",
    "        model_ans = ls_var_ans_preds[var]\n",
    "        model_rationale = ls_var_rationale[var]\n",
    "        model_gold_rationale = ls_var_gold_rationales[var]\n",
    "\n",
    "        ans_resp = eval_answer(question, model_ans, ans_for_var)\n",
    "        rationale_resp = eval_rationale(question, model_rationale, rationale_for_var)\n",
    "        gold_rationale_resp = eval_rationale(question, model_gold_rationale, rationale_for_var)\n",
    "\n",
    "        save_dict = {\n",
    "            'Model Answer': model_ans,\n",
    "            'Model Rationale': model_rationale,\n",
    "            'Model Gold Rationale': model_gold_rationale,\n",
    "            'Actual Answer': ans_for_var,\n",
    "            'Actual Rationale': rationale_for_var,\n",
    "            'Answer Response': ans_resp,\n",
    "            'Rationale Response': rationale_resp,\n",
    "            'Gold Rationale Response': gold_rationale_resp\n",
    "        }\n",
    "\n",
    "        with open(f'Evaluation/Eval_Results/Eval_Model_{eval_model}_Judge_Model_{judge_model}_Mode_{mode}/{file_name}_{var}.json', 'w') as f:\n",
    "            json.dump(save_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
